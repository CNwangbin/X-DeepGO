{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad45569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import click as ck\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import sys\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef\n",
    "from scipy.spatial import distance\n",
    "from scipy import sparse\n",
    "from matplotlib import pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcb4509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_data_pkl': '/Users/robin/xbiome/datasets/protein/train_data.pkl',\n",
       " 'terms_pkl': '/Users/robin/xbiome/datasets/protein/terms.pkl',\n",
       " 'test_data_pkl': '/Users/robin/xbiome/datasets/protein/test_data.pkl',\n",
       " 'go_obo': '/Users/robin/xbiome/datasets/protein/go.obo',\n",
       " 'predictions_pkl': '/Users/robin/xbiome/datasets/protein/predictions.pkl',\n",
       " 'swissprot_pkl': '/Users/robin/xbiome/datasets/protein/swissprot.pkl',\n",
       " 'test_diamond_res': '/Users/robin/xbiome/datasets/protein/test_diamond.res',\n",
       " 'uniprot_sprot_dat': '/Users/robin/xbiome/datasets/protein/uniprot_sprot.dat.gz'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_path = r'/Users/robin/xbiome/datasets/protein'\n",
    "# 存储所有数据文件路径\n",
    "data_ls = os.walk(base_path).__next__()[2]\n",
    "data_path_dict = {}\n",
    "for data in data_ls:\n",
    "    file_name = data.split('.')[0] + '_' + data.split('.')[1]\n",
    "    data_path_dict[file_name] = os.path.join(base_path, data)\n",
    "\n",
    "# Minimum number of annotated proteins in each GO annotation\n",
    "min_count = 50\n",
    "\n",
    "# Maximum number of sequence\n",
    "MAXLEN = 2000\n",
    "\n",
    "# GO subontology (bp, mf, cc)\n",
    "onts = ['mf', 'bp', 'cc']\n",
    "\n",
    "data_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65186eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ontology(object):\n",
    "    def __init__(self, filename='data/go.obo', with_rels=False):\n",
    "        self.ont = self.load(filename, with_rels)\n",
    "        self.ic = None\n",
    "\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def get_term(self, term_id):\n",
    "        if self.has_term(term_id):\n",
    "            return self.ont[term_id]\n",
    "        return None\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "\n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "\n",
    "    def load(self, filename, with_rels):\n",
    "        ont = dict()\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['regulates'] = list()\n",
    "                    obj['alt_ids'] = list()\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                elif line == '[Typedef]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = None\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(\": \")\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships\n",
    "                        obj['is_a'].append(it[1])\n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "            if obj is not None:\n",
    "                ont[obj['id']] = obj\n",
    "        for term_id in list(ont.keys()):\n",
    "            for t_id in ont[term_id]['alt_ids']:\n",
    "                ont[t_id] = ont[term_id]\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                del ont[term_id]\n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "        return ont\n",
    "\n",
    "    def get_anchestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while (len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        return term_set\n",
    "\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        for parent_id in self.ont[term_id]['is_a']:\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        return self.ont[term_id]['namespace']\n",
    "\n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd391c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOLOGICAL_PROCESS = 'GO:0008150'\n",
    "MOLECULAR_FUNCTION = 'GO:0003674'\n",
    "CELLULAR_COMPONENT = 'GO:0005575'\n",
    "\n",
    "FUNC_DICT = {\n",
    "    'cc': CELLULAR_COMPONENT,\n",
    "    'mf': MOLECULAR_FUNCTION,\n",
    "    'bp': BIOLOGICAL_PROCESS}\n",
    "\n",
    "NAMESPACES = {\n",
    "    'cc': 'cellular_component',\n",
    "    'mf': 'molecular_function',\n",
    "    'bp': 'biological_process'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5553727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc(labels, preds):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "\n",
    "def compute_mcc(labels, preds):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    mcc = matthews_corrcoef(labels.flatten(), preds.flatten())\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f201dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_annotations(go, real_annots, pred_annots):\n",
    "    total = 0\n",
    "    p = 0.0\n",
    "    r = 0.0\n",
    "    p_total= 0\n",
    "    ru = 0.0\n",
    "    mi = 0.0\n",
    "    fps = []\n",
    "    fns = []\n",
    "    for i in range(len(real_annots)):\n",
    "        if len(real_annots[i]) == 0:\n",
    "            continue\n",
    "        tp = set(real_annots[i]).intersection(set(pred_annots[i]))\n",
    "        fp = pred_annots[i] - tp\n",
    "        fn = real_annots[i] - tp\n",
    "        for go_id in fp:\n",
    "            mi += go.get_ic(go_id)\n",
    "        for go_id in fn:\n",
    "            ru += go.get_ic(go_id)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "        tpn = len(tp)\n",
    "        fpn = len(fp)\n",
    "        fnn = len(fn)\n",
    "        total += 1\n",
    "        recall = tpn / (1.0 * (tpn + fnn))\n",
    "        r += recall\n",
    "        if len(pred_annots[i]) > 0:\n",
    "            p_total += 1\n",
    "            precision = tpn / (1.0 * (tpn + fpn))\n",
    "            p += precision\n",
    "    ru /= total\n",
    "    mi /= total\n",
    "    r /= total\n",
    "    if p_total > 0:\n",
    "        p /= p_total\n",
    "    f = 0.0\n",
    "    if p + r > 0:\n",
    "        f = 2 * p * r / (p + r)\n",
    "    s = math.sqrt(ru * ru + mi * mi)\n",
    "    return f, p, r, s, ru, mi, fps, fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3cfe4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fmax():\n",
    "    fmax = 0.0\n",
    "    tmax = 0.0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    smin = 1000000.0\n",
    "    rus = []\n",
    "    mis = []\n",
    "    for t in range(1, 101, 10): # the range in this loop has influence in the AUPR output\n",
    "        threshold = t / 100.0\n",
    "        preds = []\n",
    "        for i, row in enumerate(test_df.itertuples()):\n",
    "            annots = set()\n",
    "            for go_id, score in deep_preds[i].items():\n",
    "                if score >= threshold:\n",
    "                    annots.add(go_id)\n",
    "\n",
    "            new_annots = set()\n",
    "            for go_id in annots:\n",
    "                new_annots |= go_rels.get_anchestors(go_id)\n",
    "            preds.append(new_annots)\n",
    "            \n",
    "        # Filter classes\n",
    "        preds = list(map(lambda x: set(filter(lambda y: y in go_set, x)), preds))\n",
    "    \n",
    "        fscore, prec, rec, s, ru, mi, fps, fns = evaluate_annotations(go_rels, labels, preds)\n",
    "        avg_fp = sum(map(lambda x: len(x), fps)) / len(fps)\n",
    "        avg_ic = sum(map(lambda x: sum(map(lambda go_id: go_rels.get_ic(go_id), x)), fps)) / len(fps)\n",
    "        # print(f'{avg_fp} {avg_ic}')\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        # print(f'Fscore: {fscore}, Precision: {prec}, Recall: {rec} S: {s}, RU: {ru}, MI: {mi} threshold: {threshold}')\n",
    "        if fmax < fscore:\n",
    "            fmax = fscore\n",
    "            tmax = threshold\n",
    "        if smin > s:\n",
    "            smin = s\n",
    "    print(f'threshold: {tmax}')\n",
    "    print(f'Smin: {smin:0.3f}')\n",
    "    print(f'Fmax: {fmax:0.3f}')\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    sorted_index = np.argsort(recalls)\n",
    "    recalls = recalls[sorted_index]\n",
    "    precisions = precisions[sorted_index]\n",
    "    aupr = np.trapz(precisions, recalls)\n",
    "    print(f'AUPR: {aupr:0.3f}')\n",
    "#     plt.figure()\n",
    "#     lw = 2\n",
    "#     plt.plot(recalls, precisions, color='darkorange',\n",
    "#              lw=lw, label=f'AUPR curve (area = {aupr:0.2f})')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.title('Area Under the Precision-Recall curve')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.savefig(f'results/aupr_{ont}_{alpha:0.2f}.pdf')\n",
    "#     df = pd.DataFrame({'precisions': precisions, 'recalls': recalls})\n",
    "#     df.to_pickle(f'results/PR_{ont}_{alpha:0.2f}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7d02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_rels = Ontology(data_path_dict['go_obo'], with_rels=True)\n",
    "terms_df = pd.read_pickle(data_path_dict['terms_pkl'])\n",
    "terms = terms_df['terms'].values.flatten()\n",
    "terms_dict = {v: i for i, v in enumerate(terms)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de1a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "train_df = pd.read_pickle(data_path_dict['train_data_pkl'])\n",
    "test_df = pd.read_pickle(data_path_dict['predictions_pkl'])\n",
    "\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc9d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = train_df['prop_annotations'].values\n",
    "annotations = list(map(lambda x: set(x), annotations))\n",
    "test_annotations = test_df['prop_annotations'].values\n",
    "test_annotations = list(map(lambda x: set(x), test_annotations))\n",
    "go_rels.calculate_ic(annotations + test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4cb7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ics = {}\n",
    "for term in terms:\n",
    "    ics[term] = go_rels.get_ic(term)\n",
    "\n",
    "prot_index = {}\n",
    "for i, row in enumerate(train_df.itertuples()):\n",
    "    prot_index[row.proteins] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48291427",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamond_scores = {}\n",
    "with open(data_path_dict['test_diamond_res']) as f:\n",
    "    for line in f:\n",
    "        it = line.strip().split()\n",
    "        if it[0] not in diamond_scores:\n",
    "            diamond_scores[it[0]] = {}\n",
    "        diamond_scores[it[0]][it[1]] = float(it[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee80ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf\n",
      "threshold: 0.11\n",
      "Smin: 14.854\n",
      "Fmax: 0.353\n",
      "AUPR: 0.286\n",
      "bp\n",
      "threshold: 0.21\n",
      "Smin: 52.248\n",
      "Fmax: 0.329\n",
      "AUPR: 0.279\n",
      "cc\n",
      "threshold: 0.31\n",
      "Smin: 12.719\n",
      "Fmax: 0.614\n",
      "AUPR: 0.614\n"
     ]
    }
   ],
   "source": [
    "# DeepGOPlus\n",
    "for ont in onts:\n",
    "    print(ont)\n",
    "    go_set = go_rels.get_namespace_terms(NAMESPACES[ont])\n",
    "    go_set.remove(FUNC_DICT[ont])\n",
    "    labels = test_df['prop_annotations'].values\n",
    "    labels = list(map(lambda x: set(filter(lambda y: y in go_set, x)), labels))\n",
    "    deep_preds = []\n",
    "\n",
    "    for i, row in enumerate(test_df.itertuples()):\n",
    "        annots_dict = {}\n",
    "        for j, score in enumerate(row.preds):\n",
    "            go_id = terms[j]\n",
    "            annots_dict[go_id] = score\n",
    "        deep_preds.append(annots_dict)\n",
    "\n",
    "    compute_fmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2be55fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('AUTHOR DeepGOPlus')\\nprint('MODEL 1')\\nprint('KEYWORDS sequence alignment.')\\nfor i, row in enumerate(test_df.itertuples()):\\n    prot_id = row.proteins\\n    for go_id, score in deep_preds[i].items():\\n        print(f'{prot_id}\\t{go_id}\\t{score:.2f}')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print('AUTHOR DeepGOPlus')\n",
    "print('MODEL 1')\n",
    "print('KEYWORDS sequence alignment.')\n",
    "for i, row in enumerate(test_df.itertuples()):\n",
    "    prot_id = row.proteins\n",
    "    for go_id, score in deep_preds[i].items():\n",
    "        print(f'{prot_id}\\t{go_id}\\t{score:.2f}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efeec24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
